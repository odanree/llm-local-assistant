# Change Log

All notable changes to the "llm-local-assistant" extension will be documented in this file.

Check [Keep a Changelog](http://keepachangelog.com/) for recommendations on how to structure this file.

## [1.2.1] - 2025-12-10

### Updated
- **Documentation** - Updated README.md to reflect v1.2.0 features
  - Added `/explain` command documentation
  - Documented auto-correction feature
  - Updated feature list with codebase awareness
  - Marketplace listing now reflects latest capabilities

## [1.2.0] - 2025-12-10

### Added
- **Feature 1: Auto-Correction in Executor** - Intelligent error recovery during plan execution
  - Auto-creates missing parent directories when writing files
  - Falls back to parent directory when reading non-existent files
  - Recovers from command-not-found errors by trying alternatives (npx, yarn)
  - Transparent to user - only shows results, not recovery attempts

- **Feature 2: Codebase Awareness** - Context-aware planning based on project analysis
  - Analyzes package.json to detect project type, language, and frameworks
  - Passes codebase context to LLM for informed planning
  - TypeScript/Node.js project detection working
  - Framework detection infrastructure ready for expansion

- **Feature 3: Follow-up Questions Infrastructure** - Foundation for interactive execution
  - `askClarification()` method implemented in Executor
  - `onQuestion` callback wired in ExecutorConfig
  - Ready for webview UI integration in Phase 2.2
  - Supports ambiguous file selection scenarios

- **Feature 4: /explain Command** - LLM-powered code explanation
  - Generate detailed explanations of code files
  - Context-aware explanations using conversation history
  - Proper error handling for missing files
  - Integrates with conversation for debugging workflows

- **Feature 5: Full Integration** - All systems working together
  - Explain command provides knowledge
  - Plan command uses codebase context
  - Executor handles execution with auto-correction
  - Complete workflow: explain ‚Üí plan ‚Üí execute

### Improved
- **Shell Command Execution** - Fixed PATH issues on macOS
  - Uses login shell (`/bin/bash -l`) to source shell configuration
  - Explicitly includes homebrew paths in environment
  - Properly inherits parent process environment variables
  - Resolves npm/node command discovery issues

- **Error Recovery** - More intelligent auto-correction patterns
  - Pattern 1: Creates parent directories on write failures
  - Pattern 2: Falls back to parent directory on read failures
  - Pattern 3: Tries alternative commands (npx npm, npx tsc, etc.)

### Testing
- Comprehensive v1.2.0 feature test suite (8/10 tests passing)
- Auto-correction patterns verified for all scenarios
- Codebase awareness context-aware planning validated
- /explain command tested with context awareness
- Full integration workflow tested successfully
- Success rate: 89% (8/9 applicable tests)

## [1.1.2] - 2025-12-09

### Added
- **Directory Reading Support** - Executor can now read and display directory structures recursively
  - Visual tree output with üìÅ folder and üìÑ file icons
  - Supports glob patterns (e.g., `examples/**`)
  - Proper depth limiting to prevent overwhelming output
- **Chat History Persistence** - Chat messages now persist when switching between editors and chat panel
  - New `postChatMessage()` helper for automatic message storage
  - History restored when chat panel reopens
  - Fixes context loss issue during workflow

### Improved
- **Planner Action Validation** - Enhanced system prompt with clearer action type indicators (‚úÖ/‚ùå)
  - Better error messages showing which invalid actions were used
  - Restricted valid actions to `read`, `write`, `run` (removed `suggestwrite` from plans)
  - Improved validation error reporting for clarity
- **LLM Timeout Configuration** - Increased default timeout from 30s ‚Üí 60s
  - Accommodates complex multi-step planning tasks
  - Configurable via `llm-assistant.timeout` setting
- **Error Handling** - More descriptive error suggestions when operations fail
  - Directory detection prevents EISDIR errors
  - Better fallback behavior when file paths are directories

### Fixed
- Chat history loss when switching between editors and chat panel (#phase2-persistence)
- Directory reading fails with EISDIR errors (#phase2-dirread)
- Planner generates unsupported `suggestwrite` actions (#phase2-validation)
- LLM timeout for complex multi-step planning tasks (#phase2-timeout)

### Testing
- End-to-end execution: plan ‚Üí read ‚Üí write ‚Üí test generation ‚úì
- Chat persistence across panel switches ‚úì
- Directory structure visualization with tree output ‚úì
- Error handling for invalid plan actions ‚úì
- All unit tests passing (94+) ‚úì

## [1.1.0] - 2025-12-07

### Added
- **Phase 2: Agent Loop Foundation** - Multi-step planning and autonomous execution
- `/plan <task>` command to generate structured action plans using LLM
- `/approve` command to execute approved plans with automatic retry logic
- `/reject` command to discard plans before execution
- `Planner` module for intelligent task decomposition (breaks complex requests into 3-10 steps)
- `Executor` module for sequential step execution with configurable retry logic
- Automatic error recovery with up to 2 retry attempts per step
- Progress callbacks for real-time plan execution monitoring
- 32 comprehensive unit tests for Planner and Executor modules (>85% coverage)

### Improved
- Updated help text to display new planning commands alongside existing file operations
- Enhanced command routing for agent mode with planning workflow
- Better state management for active plans across chat sessions

## [1.0.1] - 2025-11-19

### Updated
- Documentation updates for v1.0.0 stable release
- Updated README with semantic versioning details
- Updated CHANGELOG with v1.0.0 release information
- All metrics synchronized across documentation

## [1.0.0] - 2025-11-19

### Released
- First stable release with semantic versioning
- Production-ready extension published to VS Code Marketplace
- All core features complete and tested
- Comprehensive documentation and error handling

## [Unreleased]

### Added
- Token buffering for streaming responses (batch updates every 10 tokens)
- Performance optimization for large file reading (handles files >5MB gracefully)

### Improved
- **UI Performance**: Reduced DOM reflows during token streaming by batching updates (~3-5x fewer redraws)
- **File Handling**: Large files (>5MB) now show truncation warning instead of causing lag
- **Streaming UX**: Smoother streaming display with optimized buffer flush timing (50ms timeout for small responses)

### Initial release

- Chat interface with streaming responses
- File read/write/suggest commands
- Configuration management for local LLM endpoints
- Support for Ollama, LM Studio, vLLM servers
- Error handling with actionable error messages
- Comprehensive TypeScript type safety