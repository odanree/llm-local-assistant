# Quick Reference Guide

## ðŸš€ Getting Started (5 min)

### 1. Start Your LLM Server
```bash
# Ollama
ollama run mistral

# LM Studio
# Download from https://lmstudio.ai and start local server
```

### 2. Install & Compile Extension
```bash
cd llm-local-assistant
npm install
npm run compile
# Or: npm run watch (for development)
```

### 3. Configure (Optional)
VS Code Settings â†’ Search "llm-assistant"
```json
{
  "llm-assistant.endpoint": "http://localhost:11434",
  "llm-assistant.model": "mistral"
}
```

### 4. Use It
- Click **LLM Assistant** button in status bar
- Type your question
- Press Enter to chat

---

## ðŸ’¬ Chat Commands

### Normal Chat
```
You: "What's the best way to handle errors in TypeScript?"
LLM: [streaming response...]
```

### Read Files
```
/read src/main.ts
â†’ Displays file content in code block
```

### Generate & Write Files
```
/write src/greeting.ts write a TypeScript greeter function
â†’ LLM generates code, writes to file
â†’ Shows success with file preview
```

### Suggest & Approve Changes
```
/suggestwrite src/config.ts add environment variable validation
â†’ LLM generates code
â†’ Shows confirmation dialog
â†’ Write only if you click "Yes"
```

---

## âš™ï¸ Configuration

| Setting | Default | Purpose |
|---------|---------|---------|
| `endpoint` | `http://localhost:11434` | LLM server URL |
| `model` | `mistral` | Model name |
| `temperature` | `0.7` | Creativity (0-1) |
| `maxTokens` | `2048` | Max response length |
| `timeout` | `30000` | Request timeout (ms) |

### Custom Ollama Port
```json
{
  "llm-assistant.endpoint": "http://127.0.0.1:9000"
}
```

---

## ðŸ”§ Development

### Build
```bash
npm run compile    # One-time build
npm run watch      # Watch mode (auto-rebuild)
npm run lint       # Check code style
```

### Debug
```bash
Press F5 in VS Code
â†’ Opens extension in new VS Code window
â†’ Debug with breakpoints, console, etc.
```

### Test LLM Connection
```bash
Ctrl+Shift+P â†’ "Test LLM Connection"
â†’ Shows success/failure message
```

---

## ðŸ› Troubleshooting

### Connection Failed
```
âœ— "Could not connect to LLM server"

Solutions:
1. Verify LLM is running: ollama run mistral
2. Check endpoint URL is correct
3. Verify firewall allows localhost access
4. Restart VS Code
```

### Model Not Found
```
âœ— "Model 'mistral' not found"

Solutions:
1. List available: ollama list
2. Pull model: ollama pull mistral
3. Update "model" setting to existing model name
```

### Timeout Errors
```
âœ— "Request timeout after 30000ms"

Solutions:
1. Increase timeout: "llm-assistant.timeout": 60000
2. Reduce maxTokens to speed up response
3. Try shorter/simpler prompt
4. Check LLM server has enough RAM
```

### Commands Not Working
```
âœ— "/read" or "/write" not responding

Solutions:
1. Open a folder in VS Code (File â†’ Open Folder)
2. Use relative paths: /read src/main.ts
3. Check path exists: /read . (reads project root)
4. Check file permissions
```

---

## ðŸ“ File Paths

### Valid Examples
```
/read src/main.ts           âœ…
/read ./src/main.ts         âœ…
/read package.json          âœ…
/read dist/extension.js     âœ…

/read /absolute/path        âŒ (must be relative)
/read nonexistent.ts        âŒ (file not found)
/read ../../../etc/passwd   âš ï¸  (outside workspace)
```

### Path Tips
- Use relative paths from workspace root
- `.` refers to workspace root
- `..` goes up one level (workspace-safe)
- Paths are case-sensitive on Linux/Mac

---

## ðŸ“Š LLM Models Tested

### âœ… Verified Working
- Mistral 7B (recommended, fast & accurate)
- Neural Chat 7B
- Orca 2 13B
- Llama 2 7B

### Performance
```
Speed:    Mistral > Neural Chat > Orca > Llama 2
Quality:  Orca > Llama 2 > Mistral > Neural Chat
Memory:   Mistral (4GB) > Neural Chat > Orca > Llama 2

Recommendation: Mistral (best balance)
```

---

## ðŸŽ¯ Common Use Cases

### Code Review
```
User: "Review this code for security issues"
     [paste code here]
LLM: [analyzes and provides feedback]
```

### Generate Boilerplate
```
/write src/utils/validation.ts create email validation utilities
â†’ LLM generates complete validation functions
```

### Understand Code
```
/read src/complex-algorithm.ts
User: "Explain this algorithm step by step"
LLM: [detailed explanation with references to code]
```

### Refactor with Approval
```
/suggestwrite src/old-code.ts modernize this to ES2020
â†’ Review suggestions
â†’ Click Yes to apply
```

---

## ðŸš€ Advanced Tips

### Using with VS Code Workspace
```bash
# Open workspace-specific LLM chat
code --extensions-dir . llm-local-assistant.code-workspace

# Create multiple VS Code instances with different LLM models
# (useful for comparing responses)
```

### Streaming Large Files
```
# For large /write operations, chat will stream in real-time
# You can see tokens appearing one by one
# Ctrl+C to cancel if needed (not yet supported, coming Phase 2)
```

### Context Awareness
```
Best Practice:
1. /read relevant files first
2. Chat about changes needed
3. Use /suggestwrite to preview
4. Use /write for final version

This way LLM understands context better
```

### Multiple Commands in One Prompt
```
You: Read the config file and generate validation. 
     /read config.ts
     /write validation.ts validate the config based on the schema
```

---

## ðŸ“š Next Steps

### Want to Contribute?
1. Read ARCHITECTURE.md (understand code structure)
2. Check ROADMAP.md (see what needs work)
3. Pick a Phase 2 item to implement
4. Submit PR with tests

### Want to Learn More?
1. Read ARCHITECTURE.md for technical details
2. Read ROADMAP.md for product vision
3. Review source code: src/extension.ts, src/llmClient.ts
4. Check PROJECT_STATUS.md for metrics & roadmap

### Want Advanced Features?
See ROADMAP.md for:
- Phase 2: Agent loop & error correction
- Phase 3: Smart debugging
- Phase 4: VS Code integration
- Phase 5: External tool APIs
- Phase 6: Mission Control UI
- Phase 7: Enterprise features

---

## ðŸŽ“ Architecture Crash Course

```
User Types in Chat
    â†“
Webview captures input
    â†“
Extension.ts processes command
    â†“
Regex checks for /read, /write, /suggestwrite
    â†“
If file operation: File I/O
If chat: LLMClient.sendMessageStream()
    â†“
HTTP â†’ Ollama/LM Studio â†’ LLM Model
    â†“
Streaming tokens back
    â†“
Webview renders in real-time
```

---

## ðŸ“ž Getting Help

### Check These First
1. README.md - General guide
2. ARCHITECTURE.md - Technical questions
3. ROADMAP.md - Planned features
4. This quick ref - Common issues

### Still Stuck?
1. Enable console logging in debugger (F12)
2. Check Extension Host output panel
3. Verify LLM server is responding: `curl localhost:11434/api/tags`
4. Try with simpler prompt/file
5. Review error message carefully (usually has a hint)

---

## ðŸŽ‰ Pro Tips

1. **Combine commands in chat context**
   - Read files, ask questions, then generate variations
   
2. **Use /suggestwrite for important changes**
   - Safer than /write, gives you time to review

3. **Test connection after config changes**
   - Cmd+Shift+P â†’ "Test LLM Connection"

4. **Keep prompts specific**
   - "Generate a React component" â†’ "Generate a React date picker component with props for minDate and maxDate"

5. **Use temperature wisely**
   - Lower (0.3): Deterministic (code generation)
   - Higher (0.9): Creative (brainstorming)

---

Last Updated: November 2025  
Status: âœ… Current & Accurate
