# Architecture Documentation

> âš ï¸ **DOCUMENTATION CONSTRAINT**: The root directory contains a fixed set of documentation files: README.md, ROADMAP.md, ARCHITECTURE.md, PROJECT_STATUS.md, QUICK_REFERENCE.md, and CHANGELOG.md. No additional .md or .txt files should be created in root. Updates should be made to existing files or placed in `/docs/` if necessary.

## Phase 3: Architecture Alignment - `.cursorrules` Injection

**NEW** (v1.3.0): The extension now supports project-specific architecture rules via `.cursorrules` files.

### How It Works

1. **Create `.cursorrules` in your project root** with team patterns:
```yaml
- Use functional components only
- Validate with Zod
- State: Zustand (not Redux)
- API: TanStack Query
```

2. **Extension loads automatically** on startup
3. **Rules injected into LLM system prompt** before every `/plan` or `/write`
4. **Generated code matches project patterns** without manual guidance

### Example

**Without `.cursorrules`** (LLM guesses):
```
User: /write src/LoginForm.tsx
LLM: Generates class component with useState + fetch
Result: âŒ Doesn't match team patterns
```

**With `.cursorrules`** (LLM follows rules):
```
User: /write src/LoginForm.tsx
LLM: Generates functional component with TanStack Query + Zod
Result: âœ… Matches team patterns automatically
```

### Setup

See `docs/CURSORRULES_EXAMPLE.md` for a complete example file to copy and customize.

---

## High-Level Overview

[Rest of existing documentation...]

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VS Code Extension                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Webview UI      â”‚  â”‚  Command Handler â”‚              â”‚
â”‚  â”‚  - Chat panel    â”‚  â”‚  - /read         â”‚              â”‚
â”‚  â”‚  - Messages      â”‚  â”‚  - /write        â”‚              â”‚
â”‚  â”‚  - Input field   â”‚  â”‚  - /suggestwrite â”‚              â”‚
â”‚  â”‚  - Streaming     â”‚  â”‚  - Chat          â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                     â”‚                        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                                 â”‚                    â”‚   â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”´â”€â”€â” â”‚
â”‚                          â”‚ LLMClient       â”‚    â”‚ File  â”‚ â”‚
â”‚                          â”‚ - sendMessage() â”‚    â”‚ Utils â”‚ â”‚
â”‚                          â”‚ - sendStream()  â”‚    â”‚       â”‚ â”‚
â”‚                          â”‚ - isHealthy()   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                    â”‚                      â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                          â”‚ HTTP Client        â”‚          â”‚
â”‚                          â”‚ (axios/fetch)      â”‚          â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                              â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚ Ollama   â”‚                   â”‚LM Studio â”‚
                   â”‚ API      â”‚                   â”‚API       â”‚
                   â”‚ (Local)  â”‚                   â”‚(Local)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Components

### 1. Extension (src/extension.ts)

**Responsibility**: Main orchestrator, command handling, webview management

**Key Functions**:
- `activate()` - Extension startup, command registration
- `openLLMChat()` - Create and manage webview panel
- Message handler for `sendMessage` command
- Agent command parsing (/read, /write, /suggestwrite)
- Error handling and user messaging

**Flow**:
```
User triggers /read, /write, /suggestwrite
    â†“
Regex parsing extracts command + params
    â†“
Command-specific handler executes
    â†“
File I/O or LLM call
    â†“
Result sent to webview via postMessage
```

### 2. LLMClient (src/llmClient.ts)

**Responsibility**: LLM communication, streaming, configuration

**Key Methods**:
- `sendMessage(prompt)` - Non-streaming request
- `sendMessageStream(prompt, callback)` - Streaming with token callback
- `isServerHealthy()` - Health check
- `clearHistory()` - Reset conversation context

**Architecture**:
```typescript
class LLMClient {
  private config: LLMConfig;
  private messageHistory: Message[];
  
  async sendMessage(text: string): Promise<{
    success: boolean;
    message?: string;
    error?: string;
  }>;
  
  async sendMessageStream(
    text: string,
    onToken: (token: string, complete: boolean) => Promise<void>
  ): Promise<{ success: boolean; error?: string }>;
}
```

**Streaming Logic**:
1. Open SSE connection to LLM endpoint
2. Listen for `data:` events
3. Parse JSON responses
4. Invoke callback for each token
5. Call callback with `complete: true` when done

### 3. Webview Content (src/webviewContent.ts)

**Responsibility**: Chat UI, message rendering, user interaction

**HTML Structure**:
```html
<div id="chat">
  <!-- Messages append here -->
</div>
<div class="input-row">
  <input id="input" type="text" placeholder="..." />
  <button id="send">Send</button>
  <button id="clear">Clear</button>
</div>
```

**Message Types**:
- `streamToken` - Single token in streaming response
- `streamComplete` - Marks end of streaming
- `addMessage` - Complete message (error, file content, etc.)
- `status` - Status message (grayed out)

**Styling**:
- User messages: Blue gradient (left align)
- Assistant messages: Gray gradient (right align)
- Error messages: Red gradient with warning icon
- Status messages: Subtle gray background
- Hover effects: transform and shadow transitions
- Focus states: Blue outline rings

---

## Command Processing Flow

### `/read <path>`

```
User enters: /read src/main.ts
    â†“
Regex: /\/read\s+(\S+)/
    â†“
Extract path: "src/main.ts"
    â†“
vscode.workspace.fs.readFile(fileUri)
    â†“
TextDecoder converts to string
    â†“
Send to webview as addMessage with file content in code block
```

### `/write <path> [prompt]`

```
User enters: /write config.ts validate this config
    â†“
Regex: /\/write\s+(\S+)(?:\s+(.+))?$/
    â†“
Extract path: "config.ts", prompt: "validate this config"
    â†“
LLMClient.sendMessageStream(prompt)
    â†“
Collect all tokens into generatedContent
    â†“
Create parent directories with createDirectory()
    â†“
vscode.workspace.fs.writeFile(fileUri, content)
    â†“
Send success message with preview to webview
```

### `/suggestwrite <path> [prompt]`

```
Similar to /write, but:
    â†“
After LLM generates content
    â†“
Show confirmation dialog with preview
    â†“
If user clicks "Yes": write file
If user clicks "No": send cancel message to webview
```

---

## Data Flow: Chat Message

```
1. User types & presses Enter
2. Webview JavaScript captures input
3. vscode.postMessage({ command: 'sendMessage', text: '...' })
4. Extension receives onDidReceiveMessage
5. Parse for agent commands (regex)
   - If command found: execute special logic
   - If normal chat: send to LLM
6. Streaming:
   - LLM returns token-by-token via SSE
   - For each token: postMessage({ command: 'streamToken', token: '...' })
   - Webview appends token to message
7. Final:
   - postMessage({ command: 'streamComplete' })
   - Webview freezes final message

```

---

## Error Handling

### Try-Catch Strategy
- All user-triggered operations wrapped in try-catch
- Errors sent as `addMessage` with `error` property
- Webview renders in red with warning icon
- Includes helpful context (file path, error message)

### Common Errors
```typescript
// File not found
Error reading file: src/missing.ts
No such file or directory

// No workspace
Error: No workspace folder open.
Suggestion: Open a folder first

// LLM timeout
Error: Request timeout after 30000ms
Suggestion: Increase timeout in settings or simplify prompt
```

---

## Configuration Management

### Settings Flow
```
package.json contributes section
    â†“
VS Code loads settings
    â†“
Extension queries at startup via getLLMConfig()
    â†“
Create LLMClient with config
    â†“
Settings available via vscode.workspace.getConfiguration()
```

### Config Validation
- Endpoint URL must be valid HTTP(S)
- Model name required
- Temperature 0-2 range
- Timeout >= 1000ms
- MaxTokens >= 100

---

## Extension Activation

```
1. VS Code starts
2. Loads extension.ts
3. activate() called
4. Register commands: openChat, testConnection
5. Create status bar item "ğŸ¤– LLM Assistant"
6. Initialize LLMClient with config
7. Ready for user interaction
```

---

## File Structure

```
llm-local-assistant/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extension.ts           # Main orchestrator (385 lines)
â”‚   â”œâ”€â”€ llmClient.ts           # LLM API client (200+ lines)
â”‚   â””â”€â”€ webviewContent.ts      # Chat UI (226 lines)
â”œâ”€â”€ dist/
â”‚   â””â”€â”€ extension.js           # Compiled output (webpack)
â”œâ”€â”€ .vscode/
â”‚   â”œâ”€â”€ launch.json            # Debug configuration
â”‚   â””â”€â”€ tasks.json             # Build tasks
â”œâ”€â”€ package.json               # Dependencies, scripts, config
â”œâ”€â”€ tsconfig.json              # TypeScript config
â”œâ”€â”€ webpack.config.js          # Bundle configuration
â”œâ”€â”€ eslint.config.mjs          # Linting rules
â”œâ”€â”€ README.md                  # User guide
â”œâ”€â”€ ROADMAP.md                 # Future features
â”œâ”€â”€ ARCHITECTURE.md            # This file
â””â”€â”€ CHANGELOG.md               # Version history
```

---

## Build Pipeline

### Development
```
npm run watch
    â†“
Webpack watches src/ for changes
    â†“
Compiles to dist/extension.js
    â†“
Auto-reload available in debugger (F5)
```

### Production
```
npm run compile
    â†“
Webpack bundles with minification
    â†“
Outputs dist/extension.js (~29KB)
    â†“
Ready for packaging/distribution
```

### TypeScript â†’ JavaScript
- Language: TypeScript (src/)
- Target: ES2020 JavaScript
- Module format: CommonJS
- Declaration files: Not generated

---

## VS Code API Usage

### Core APIs Used

| API | Purpose | Used For |
|-----|---------|----------|
| `vscode.window.createWebviewPanel()` | Create chat UI | Chat window |
| `webview.postMessage()` | Send to webview | Update UI |
| `webview.onDidReceiveMessage()` | Receive from UI | Handle user actions |
| `vscode.workspace.fs` | File I/O | /read, /write |
| `vscode.window.showInformationMessage()` | Dialogs | /suggestwrite approval |
| `vscode.commands.registerCommand()` | Commands | openChat, testConnection |
| `vscode.window.createStatusBarItem()` | Status bar | Quick access button |
| `vscode.workspace.getConfiguration()` | Settings | Get config values |
| `vscode.Uri.joinPath()` | Path handling | Build file URIs |

---

## Security Considerations

### Current Safeguards
- No `eval()` or `innerHTML` with user input
- File operations limited to workspace folder
- Commands registered by extension only
- No shell access (yet)
- No internet access required

### Future Security Needs (Agent Mode)
- Whitelist allowed shell commands
- Audit logging for file operations
- Permission system for external tools
- Sandboxing for tool execution

---

## Performance Optimizations

### Current
- Streaming reduces perceived latency
- Lazy webview creation
- Single LLMClient instance (connection pooling)
- CSS transitions for smooth UX

### Future
- Context windowing for large files
- Incremental file indexing
- Token caching for common patterns
- Background worker for heavy parsing

---

## Testing Strategy

### Unit Tests
- LLMClient methods
- Regex command parsing
- File path normalization
- Configuration validation

### Integration Tests
- End-to-end message flow
- File I/O with error handling
- Streaming response handling
- Webview communication

### Manual Testing
- Chat with different models
- File operations (/read, /write, /suggestwrite)
- Error scenarios (offline, missing files)
- Custom port configurations

---

## Debugging Tips

### Enable Debug Logging
```typescript
// In extension.ts
console.log('[LLM Assistant]', message);
```

View in VS Code "Output" panel â†’ "Extension Host"

### Common Issues

| Issue | Debug Step |
|-------|-----------|
| "Cannot connect" | Check LLM server running: `curl http://localhost:11434/api/tags` |
| "Model not found" | Verify model: `ollama list` |
| "Timeout" | Increase `llm-assistant.timeout` in settings |
| "No workspace" | Open a folder before using /read, /write |
| Chat not responding | Check Network tab in DevTools (F12 in debug host) |

---

Last Updated: November 2025
