# v2.6: Voice Narration Implementation Plan

## Overview

Add voice narration to `/explain` command using local ChatTTS model. Zero external APIs, runs entirely offline.

**Timeline:** 2 weeks  
**Effort:** ~40 hours (distributed)  
**Target Release:** Mid-March 2026

---

## Phase 1: ChatTTS Python Service (Days 1-3)

### 1.1 Create Python TTS Service

**File:** `python/tts_service.py`

```python
import sys
import json
import torch
import io
import wave
from typing import Tuple
from pathlib import Path

class ChatTTSService:
    def __init__(self, device: str = None):
        """Initialize ChatTTS model"""
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device
        self.model = None
        self.sample_rate = 24000  # ChatTTS standard
        
    def load_model(self):
        """Load ChatTTS model from HuggingFace"""
        try:
            from ChatTTS import ChatTTS
            self.model = ChatTTS.load(device=self.device)
            return True
        except Exception as e:
            print(json.dumps({"error": str(e)}), file=sys.stderr)
            return False
    
    def synthesize(self, text: str, lang: str = 'en') -> Tuple[int, bytes]:
        """
        Generate audio from text
        
        Args:
            text: Text to synthesize
            lang: Language ('en' or 'zh')
        
        Returns:
            Tuple of (sample_rate, audio_bytes)
        """
        if self.model is None:
            self.load_model()
        
        try:
            # Generate audio (returns numpy array)
            wav = self.model.infer(text, lang=lang, use_decoder=True)
            
            # Convert to WAV bytes
            audio_bytes = self._numpy_to_wav(wav[0], self.sample_rate)
            return self.sample_rate, audio_bytes
        except Exception as e:
            raise Exception(f"TTS synthesis failed: {str(e)}")
    
    def _numpy_to_wav(self, audio: 'np.ndarray', sample_rate: int) -> bytes:
        """Convert numpy audio array to WAV bytes"""
        import numpy as np
        
        # Normalize audio to [-1, 1] range
        audio = np.clip(audio, -1.0, 1.0)
        
        # Convert to 16-bit PCM
        audio_int16 = np.int16(audio * 32767)
        
        # Create WAV file in memory
        wav_buffer = io.BytesIO()
        with wave.open(wav_buffer, 'wb') as wav_file:
            wav_file.setnchannels(1)  # Mono
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_int16.tobytes())
        
        return wav_buffer.getvalue()

def main():
    """CLI interface for TTS service"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ChatTTS Service')
    parser.add_argument('--text', required=True, help='Text to synthesize')
    parser.add_argument('--lang', default='en', help='Language (en or zh)')
    parser.add_argument('--device', default=None, help='Device (cuda or cpu)')
    parser.add_argument('--info', action='store_true', help='Return device info')
    
    args = parser.parse_args()
    
    service = ChatTTSService(device=args.device)
    
    if args.info:
        print(json.dumps({
            "device": service.device,
            "cuda_available": torch.cuda.is_available(),
            "sample_rate": service.sample_rate
        }))
        return
    
    try:
        sample_rate, audio_bytes = service.synthesize(args.text, lang=args.lang)
        # Write audio bytes to stdout
        sys.stdout.buffer.write(audio_bytes)
        # Write metadata to stderr
        print(json.dumps({
            "sample_rate": sample_rate,
            "size": len(audio_bytes)
        }), file=sys.stderr)
    except Exception as e:
        print(json.dumps({"error": str(e)}), file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
```

### 1.2 Create Setup Script

**File:** `python/setup_tts.py`

```python
"""
Setup ChatTTS model and dependencies
Run: python python/setup_tts.py
"""

import subprocess
import sys

def setup():
    print("Setting up ChatTTS for voice narration...")
    
    # Install dependencies
    print("\n1. Installing Python dependencies...")
    subprocess.run([
        sys.executable, '-m', 'pip', 'install',
        'ChatTTS',
        'numpy',
        'torch',
        'torchaudio'
    ], check=True)
    
    print("\n2. Downloading ChatTTS model (~1GB)...")
    print("   This may take a few minutes on first run...")
    
    # Import and load model to cache it
    try:
        from ChatTTS import ChatTTS
        model = ChatTTS.load()
        print("‚úÖ ChatTTS model loaded and cached successfully!")
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        sys.exit(1)
    
    print("\n‚úÖ Setup complete! Voice narration is ready to use.")

if __name__ == '__main__':
    setup()
```

### 1.3 Test ChatTTS Service

**File:** `python/test_tts.py`

```python
"""
Quick test of TTS service
Run: python python/test_tts.py
"""

from tts_service import ChatTTSService
import wave

def test():
    print("Testing ChatTTS service...")
    
    service = ChatTTSService()
    test_text = "Hello! This is a test of the voice narration feature."
    
    print(f"Synthesizing: {test_text}")
    sample_rate, audio_bytes = service.synthesize(test_text)
    
    # Save to test file
    with open('test_audio.wav', 'wb') as f:
        f.write(audio_bytes)
    
    print(f"‚úÖ Audio generated successfully!")
    print(f"   Sample rate: {sample_rate} Hz")
    print(f"   File size: {len(audio_bytes)} bytes")
    print(f"   Saved to: test_audio.wav")

if __name__ == '__main__':
    test()
```

---

## Phase 2: Node.js TTS Bridge (Days 4-6)

### 2.1 Create TTS Service Wrapper

**File:** `src/services/ttsService.ts`

```typescript
import { spawn } from 'child_process';
import * as path from 'path';
import * as fs from 'fs';

export interface TTSOptions {
  text: string;
  lang?: 'en' | 'zh';
  maxChunkLength?: number; // Split long text into chunks
}

export interface TTSResult {
  audioBuffer: Buffer;
  sampleRate: number;
  duration: number;
}

export class TTSService {
  private pythonScriptPath: string;
  private modelLoaded: boolean = false;

  constructor() {
    this.pythonScriptPath = path.join(
      __dirname,
      '../../python/tts_service.py'
    );
  }

  /**
   * Check device info (GPU available, etc)
   */
  async getDeviceInfo(): Promise<{
    device: 'cuda' | 'cpu';
    cudaAvailable: boolean;
    sampleRate: number;
  }> {
    return new Promise((resolve, reject) => {
      const python = spawn('python', [
        this.pythonScriptPath,
        '--info'
      ]);

      let stdout = '';
      let stderr = '';

      python.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      python.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      python.on('close', (code) => {
        if (code !== 0) {
          reject(new Error(`TTS setup failed: ${stderr}`));
          return;
        }

        try {
          const info = JSON.parse(stderr);
          resolve({
            device: info.device,
            cudaAvailable: info.cuda_available,
            sampleRate: info.sample_rate
          });
        } catch (e) {
          reject(new Error('Failed to parse device info'));
        }
      });
    });
  }

  /**
   * Synthesize speech from text
   * Handles chunking for long text automatically
   */
  async synthesize(options: TTSOptions): Promise<TTSResult> {
    const {
      text,
      lang = 'en',
      maxChunkLength = 500
    } = options;

    // For very long text, split into chunks
    if (text.length > maxChunkLength) {
      return this.synthesizeChunked(text, lang, maxChunkLength);
    }

    return this.synthesizeChunk(text, lang);
  }

  /**
   * Synthesize a single chunk of text
   */
  private synthesizeChunk(text: string, lang: string): Promise<TTSResult> {
    return new Promise((resolve, reject) => {
      const python = spawn('python', [
        this.pythonScriptPath,
        '--text', text,
        '--lang', lang
      ]);

      const audioChunks: Buffer[] = [];
      let stderr = '';
      let sampleRate = 24000;

      python.stdout.on('data', (chunk) => {
        audioChunks.push(chunk);
      });

      python.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      python.on('close', (code) => {
        if (code !== 0) {
          reject(new Error(`TTS synthesis failed: ${stderr}`));
          return;
        }

        try {
          const metadata = JSON.parse(stderr);
          sampleRate = metadata.sample_rate;
          const audioBuffer = Buffer.concat(audioChunks);
          const duration = (audioBuffer.length / 2) / sampleRate; // 16-bit audio

          resolve({
            audioBuffer,
            sampleRate,
            duration
          });
        } catch (e) {
          reject(new Error(`Failed to parse TTS metadata: ${e}`));
        }
      });

      python.on('error', (err) => {
        reject(err);
      });
    });
  }

  /**
   * Synthesize long text by splitting into chunks
   * Concatenates audio from all chunks
   */
  private async synthesizeChunked(
    text: string,
    lang: string,
    chunkLength: number
  ): Promise<TTSResult> {
    // Split text intelligently (at sentence boundaries)
    const chunks = this.splitText(text, chunkLength);
    const audioChunks: Buffer[] = [];
    let sampleRate = 24000;

    for (const chunk of chunks) {
      const result = await this.synthesizeChunk(chunk, lang);
      audioChunks.push(result.audioBuffer);
      sampleRate = result.sampleRate;
    }

    // Concatenate all audio chunks
    const audioBuffer = Buffer.concat(audioChunks);
    const duration = (audioBuffer.length / 2) / sampleRate;

    return { audioBuffer, sampleRate, duration };
  }

  /**
   * Split text at sentence boundaries
   */
  private splitText(text: string, maxLength: number): string[] {
    const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];
    const chunks: string[] = [];
    let currentChunk = '';

    for (const sentence of sentences) {
      if ((currentChunk + sentence).length > maxLength) {
        if (currentChunk) chunks.push(currentChunk.trim());
        currentChunk = sentence;
      } else {
        currentChunk += sentence;
      }
    }

    if (currentChunk) chunks.push(currentChunk.trim());
    return chunks;
  }
}

export const ttsService = new TTSService();
```

### 2.2 Integrate with `/explain` Command

**File:** `src/commands/explain.ts` (modified)

```typescript
import { ttsService } from '../services/ttsService';
import * as fs from 'fs';
import * as path from 'path';

export async function explainCommand(
  args: string,
  context: any
): Promise<void> {
  // ... existing explain logic ...
  const explanation = await generateExplanation(args);

  // NEW: Check if user wants narration
  const narrate = context.config?.enableVoiceNarration !== false;

  if (narrate) {
    try {
      const audioResult = await ttsService.synthesize({
        text: explanation,
        lang: 'en'
      });

      // Save audio file
      const audioDir = path.join(context.workspaceRoot, '.llm-cache/audio');
      if (!fs.existsSync(audioDir)) {
        fs.mkdirSync(audioDir, { recursive: true });
      }

      const audioPath = path.join(
        audioDir,
        `explain-${Date.now()}.wav`
      );
      fs.writeFileSync(audioPath, audioResult.audioBuffer);

      // Return explanation + audio info
      context.panel.webview.postMessage({
        command: 'showExplanation',
        text: explanation,
        audio: {
          path: audioPath,
          sampleRate: audioResult.sampleRate,
          duration: audioResult.duration
        }
      });
    } catch (error) {
      // Fallback: just show text if TTS fails
      context.panel.webview.postMessage({
        command: 'showExplanation',
        text: explanation,
        error: 'Voice narration failed, showing text only'
      });
    }
  } else {
    context.panel.webview.postMessage({
      command: 'showExplanation',
      text: explanation
    });
  }
}
```

---

## Phase 3: Webview UI (Days 7-10)

### 3.1 Create Audio Player Component

**File:** `src/webview/components/AudioPlayer.tsx`

```typescript
import React, { useRef, useState } from 'react';

interface AudioPlayerProps {
  audioPath: string;
  duration: number;
  sampleRate: number;
}

export const AudioPlayer: React.FC<AudioPlayerProps> = ({
  audioPath,
  duration,
  sampleRate
}) => {
  const audioRef = useRef<HTMLAudioElement>(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const [playbackRate, setPlaybackRate] = useState(1);

  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  };

  const handlePlay = () => {
    if (audioRef.current) {
      if (isPlaying) {
        audioRef.current.pause();
      } else {
        audioRef.current.play();
      }
      setIsPlaying(!isPlaying);
    }
  };

  const handleSpeedChange = (newRate: number) => {
    setPlaybackRate(newRate);
    if (audioRef.current) {
      audioRef.current.playbackRate = newRate;
    }
  };

  return (
    <div className="audio-player">
      <audio
        ref={audioRef}
        src={`file://${audioPath}`}
        onTimeUpdate={(e) => setCurrentTime(e.currentTarget.currentTime)}
        onEnded={() => setIsPlaying(false)}
      />

      <div className="player-controls">
        {/* Play/Pause Button */}
        <button
          onClick={handlePlay}
          className="control-btn"
          aria-label={isPlaying ? 'Pause narration' : 'Play narration'}
        >
          {isPlaying ? '‚è∏Ô∏è Pause' : '‚ñ∂Ô∏è Play'}
        </button>

        {/* Time Display */}
        <span className="time-display">
          {formatTime(currentTime)} / {formatTime(duration)}
        </span>

        {/* Speed Controls */}
        <div className="speed-controls">
          <label htmlFor="speed">Speed:</label>
          <select
            id="speed"
            value={playbackRate}
            onChange={(e) => handleSpeedChange(parseFloat(e.target.value))}
          >
            <option value={0.75}>0.75x</option>
            <option value={1}>1x (Normal)</option>
            <option value={1.25}>1.25x</option>
            <option value={1.5}>1.5x</option>
            <option value={2}>2x</option>
          </select>
        </div>

        {/* Volume Control */}
        <input
          type="range"
          min="0"
          max="100"
          defaultValue="70"
          onChange={(e) => {
            if (audioRef.current) {
              audioRef.current.volume = parseInt(e.target.value) / 100;
            }
          }}
          aria-label="Volume"
          className="volume-control"
        />
      </div>

      {/* Progress Bar */}
      <input
        type="range"
        min="0"
        max={duration}
        value={currentTime}
        onChange={(e) => {
          const newTime = parseFloat(e.target.value);
          setCurrentTime(newTime);
          if (audioRef.current) {
            audioRef.current.currentTime = newTime;
          }
        }}
        className="progress-bar"
        aria-label="Audio progress"
      />
    </div>
  );
};
```

### 3.2 Explanation Panel with Audio

**File:** `src/webview/panels/ExplanationPanel.tsx` (modified)

```typescript
import React, { useState } from 'react';
import { AudioPlayer } from '../components/AudioPlayer';

interface ExplanationPanelProps {
  text: string;
  audio?: {
    path: string;
    sampleRate: number;
    duration: number;
  };
  error?: string;
}

export const ExplanationPanel: React.FC<ExplanationPanelProps> = ({
  text,
  audio,
  error
}) => {
  return (
    <div className="explanation-panel">
      {error && (
        <div className="error-banner">
          ‚ö†Ô∏è {error}
        </div>
      )}

      {audio && (
        <div className="audio-section">
          <h3>üéß Narration</h3>
          <AudioPlayer {...audio} />
        </div>
      )}

      <div className="explanation-text">
        <h3>üìù Explanation</h3>
        <pre>{text}</pre>
      </div>
    </div>
  );
};
```

### 3.3 Styles for Audio Player

**File:** `src/webview/styles/audioPlayer.css`

```css
.audio-player {
  background: #f5f5f5;
  border-radius: 8px;
  padding: 16px;
  margin-bottom: 16px;
}

.player-controls {
  display: flex;
  align-items: center;
  gap: 12px;
  margin-bottom: 12px;
  flex-wrap: wrap;
}

.control-btn {
  padding: 8px 16px;
  background: #0e639c;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  font-weight: 500;
}

.control-btn:hover {
  background: #1177bb;
}

.control-btn:active {
  opacity: 0.8;
}

.time-display {
  font-family: monospace;
  font-size: 12px;
  color: #666;
  min-width: 70px;
}

.speed-controls {
  display: flex;
  align-items: center;
  gap: 8px;
}

.speed-controls label {
  font-size: 12px;
  color: #666;
}

.speed-controls select {
  padding: 4px 8px;
  border: 1px solid #ccc;
  border-radius: 4px;
  background: white;
  cursor: pointer;
  font-size: 12px;
}

.volume-control {
  width: 80px;
  cursor: pointer;
}

.progress-bar {
  width: 100%;
  height: 4px;
  cursor: pointer;
  -webkit-appearance: none;
  appearance: none;
  background: #ddd;
  border-radius: 2px;
  outline: none;
}

.progress-bar::-webkit-slider-thumb {
  -webkit-appearance: none;
  appearance: none;
  width: 12px;
  height: 12px;
  border-radius: 50%;
  background: #0e639c;
  cursor: pointer;
}

.progress-bar::-moz-range-thumb {
  width: 12px;
  height: 12px;
  border-radius: 50%;
  background: #0e639c;
  cursor: pointer;
  border: none;
}

/* Accessibility: Keyboard focus */
.control-btn:focus,
.progress-bar:focus,
.speed-controls select:focus,
.volume-control:focus {
  outline: 2px solid #0e639c;
  outline-offset: 2px;
}

/* Dark mode support */
@media (prefers-color-scheme: dark) {
  .audio-player {
    background: #2d2d2d;
  }

  .time-display,
  .speed-controls label {
    color: #aaa;
  }

  .speed-controls select {
    background: #1e1e1e;
    color: #e0e0e0;
    border-color: #444;
  }

  .progress-bar {
    background: #444;
  }
}
```

---

## Phase 4: Configuration & Settings (Days 11-12)

### 4.1 Add Settings to `package.json`

```json
{
  "contributes": {
    "configuration": {
      "title": "LLM Local Assistant",
      "properties": {
        "llm-assistant.voice.enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable voice narration for /explain and other commands"
        },
        "llm-assistant.voice.speed": {
          "type": "number",
          "default": 1.0,
          "minimum": 0.5,
          "maximum": 2.0,
          "description": "Default playback speed for narration (0.5x - 2.0x)"
        },
        "llm-assistant.voice.language": {
          "type": "string",
          "enum": ["en", "zh"],
          "default": "en",
          "description": "Language for text-to-speech synthesis"
        },
        "llm-assistant.voice.maxChunkLength": {
          "type": "number",
          "default": 500,
          "description": "Maximum characters per TTS chunk (prevents long wait times)"
        }
      }
    },
    "commands": [
      {
        "command": "llm-assistant.setup-voice",
        "title": "LLM: Setup Voice Narration",
        "description": "Download and initialize ChatTTS model for voice narration"
      },
      {
        "command": "llm-assistant.test-voice",
        "title": "LLM: Test Voice Narration",
        "description": "Test text-to-speech with sample text"
      }
    ]
  }
}
```

### 4.2 Setup Command Handler

**File:** `src/commands/setupVoice.ts`

```typescript
import { execFile } from 'child_process';
import * as path from 'path';
import { window } from 'vscode';

export async function setupVoiceCommand(): Promise<void> {
  const progress = window.withProgress(
    {
      location: 10, // Notification area
      title: 'Setting up voice narration...',
      cancellable: false
    },
    async (progress) => {
      return new Promise((resolve, reject) => {
        const setupScript = path.join(
          __dirname,
          '../../python/setup_tts.py'
        );

        const python = execFile('python', [setupScript]);
        let output = '';

        python.stdout?.on('data', (data) => {
          output += data.toString();
          console.log(data.toString());
        });

        python.stderr?.on('data', (data) => {
          console.log(data.toString());
        });

        python.on('close', (code) => {
          if (code === 0) {
            window.showInformationMessage(
              '‚úÖ Voice narration is ready to use! Try /explain with narration enabled.'
            );
            resolve(undefined);
          } else {
            window.showErrorMessage(
              '‚ùå Voice narration setup failed. Check extension output for details.'
            );
            reject(new Error('Setup script failed'));
          }
        });

        python.on('error', (err) => {
          reject(err);
        });
      });
    }
  );
}
```

---

## Phase 5: Testing & Documentation (Days 13-14)

### 5.1 Test Cases

```typescript
// src/test/tts.test.ts
import { ttsService } from '../services/ttsService';

describe('TTS Service', () => {
  test('synthesizes simple text', async () => {
    const result = await ttsService.synthesize({
      text: 'Hello world'
    });
    expect(result.audioBuffer.length).toBeGreaterThan(0);
    expect(result.sampleRate).toBe(24000);
  });

  test('handles long text with chunking', async () => {
    const longText = 'This is a very long explanation. '.repeat(50);
    const result = await ttsService.synthesize({
      text: longText,
      maxChunkLength: 100
    });
    expect(result.audioBuffer.length).toBeGreaterThan(0);
    expect(result.duration).toBeGreaterThan(10); // Long audio
  });

  test('handles language switching', async () => {
    const result = await ttsService.synthesize({
      text: 'Hello',
      lang: 'en'
    });
    expect(result.sampleRate).toBe(24000);
  });
});
```

### 5.2 Setup Documentation

**File:** `docs/VOICE_NARRATION.md`

```markdown
# Voice Narration Setup Guide

## Overview

v2.6 adds optional voice narration to `/explain` command using local ChatTTS model.

## Requirements

- Python 3.8+
- GPU optional (but recommended for faster synthesis)
- ~1GB disk space for ChatTTS model
- ~2GB RAM minimum

## Installation

### 1. Initial Setup

Run the setup command from VS Code:

1. Open Command Palette (Cmd+Shift+P / Ctrl+Shift+P)
2. Search: "LLM: Setup Voice Narration"
3. Click and wait for download (~5-10 minutes first time)

Or manually:
```bash
cd /path/to/extension
python python/setup_tts.py
```

### 2. Configuration

Open VS Code Settings and search for "llm-assistant.voice":

- **Enabled**: Toggle voice narration on/off
- **Speed**: Default playback speed (0.5x - 2.0x)
- **Language**: en (English) or zh (Chinese)
- **Max Chunk Length**: Longer = higher latency, shorter = more parts

### 3. Usage

Use `/explain` command normally. Audio will be generated automatically.

## Performance

- **First run**: ~2-3 seconds (model loading)
- **Subsequent runs**: <1 second per 100 characters
- **Hardware impact**: GPU ~50% faster, CPU still acceptable
- **Battery**: Negligible impact

## Troubleshooting

### "Python not found"
Ensure Python is in your PATH:
```bash
python --version  # Should show 3.8+
```

### "ChatTTS not found"
Re-run setup:
```bash
python python/setup_tts.py
```

### Audio quality issues
- Clear pronunciation: Speak naturally in your code samples
- Volume: Adjust in player or system settings
- Speed: Try different playback speeds (0.75x - 1.5x)

### Performance issues
- Use CPU if GPU is overloaded
- Reduce max chunk length in settings
- Close other GPU applications
```

---

## Commit Strategy

### Commit 1: Python TTS Service
```
feat(tts): Add ChatTTS Python service for voice narration

- Implement ChatTTSService with synthesis API
- Add text chunking for long content
- Create setup script for model download
- Add test utilities
```

### Commit 2: Node.js Bridge
```
feat(tts): Integrate TTS service with VS Code extension

- Create TTSService wrapper with subprocess management
- Add text splitting at sentence boundaries
- Integrate with /explain command
- Add config options for voice settings
```

### Commit 3: Webview UI
```
feat(ui): Add audio player component to explanation panel

- Implement AudioPlayer with Play/Pause/Speed controls
- Add progress bar and time display
- Support volume adjustment
- Add dark mode and accessibility
```

### Commit 4: Configuration & Commands
```
feat(config): Add voice narration settings and commands

- Add configuration options to package.json
- Implement /setup-voice command
- Implement /test-voice command
- Add default settings
```

### Commit 5: Documentation & Tests
```
docs(voice): Add comprehensive voice narration documentation

- Add voice narration setup guide
- Add troubleshooting section
- Add performance notes
- Add test coverage
```

---

## Success Criteria

- ‚úÖ `/explain` generates audio automatically
- ‚úÖ Audio player has play/pause/speed controls
- ‚úÖ Long explanations split into chunks automatically
- ‚úÖ Setup command works for all platforms (Mac/Windows/Linux)
- ‚úÖ Voice narration can be toggled on/off
- ‚úÖ No external dependencies (fully local)
- ‚úÖ Docs cover setup, usage, troubleshooting
- ‚úÖ Works offline (no API calls)

---

## Post-v2.6 Ideas

**v2.7+ Voice Features:**
- Voice cloning (use Chatterbox TTS)
- Emotion control for narration
- Multiple voice options
- Real-time streaming (don't wait for full synthesis)
- `/explain --voice-only` (skip text, audio only)
- Custom pronunciations for code terms

**Integration with other commands:**
- `/refactor` with narration
- `/rate-architecture` with narration
- `/suggest-patterns` with narration

---

## Timeline

| Week | Phase | Tasks | Status |
|------|-------|-------|--------|
| 1 | Setup | ChatTTS service + testing | Days 1-3 |
| 1-2 | Bridge | Node.js integration | Days 4-6 |
| 2 | UI | Audio player component | Days 7-10 |
| 2 | Config | Settings + commands | Days 11-12 |
| 2-3 | Docs | Testing + documentation | Days 13-14 |

**Target Release:** Mid-March 2026
